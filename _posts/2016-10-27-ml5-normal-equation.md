---
layout: post
title:  "机器学习（五）正规方程、多项式回归"
image: ''
date:   2016-10-27 18:46:00
tags:
- computer science
- machine learning
- polynomial regression
- normal equation
description: '机器学习Coursera学习笔记：第五部分 正规方程、多项式回归'
categories:
- Machine Learning
twitter_text: '机器学习Coursera学习笔记：第五部分 正规方程、多项式回归'
---

## 正规方程

我们已经知道了如何利用梯度下降法求解线性回归方程的最佳参数，现在我们介绍一种更直接的方法——正规方程法。

### 正规方程

$$
\theta = (X^{T}X)^{-1}X^{T}\vec{y}
$$

### 推导与解释

我们回忆一下代价函数的向量化形式：

$$
J(\theta) = \dfrac {1}{2m} (X\theta - \vec{y})^{T} (X\theta - \vec{y})
$$

我们的目标是使其最小，那么我们计算其导数为0时的参数值，也就得到了极值点。

因为我们将要对其求导并与0作比较，所以可以抛弃前面的系数：


$$
J(\theta) = (X\theta - \vec{y})^{T} (X\theta - \vec{y})
$$

将其展开：

$$
J(\theta) = ((X\theta)^{T} - \vec{y}^{T}) (X\theta - \vec{y})
          = (X\theta)^{T} X\theta - (X\theta)^{T}\vec{y} - \vec{y}^{T}(X\theta) + \vec{y}^{T}\vec{y}
$$

对其求导，并解方程：

$$
\frac{\partial J(\theta)}{\partial \theta} = 2X^{T}X\theta - 2X^{T}\vec{y} = 0
$$

化简为：

$$
X^{T}X\theta = X^T\vec{y}
$$

假设$$X^{T}X$$可逆（列向量互相独立），等式两边同时乘以它的逆，则得到：
$$
\theta = (X^{T}X)^{-1}X^{T}\vec{y}
$$

这样，只需要直接进行矩阵运算就可以得到最佳参数的值。

另外，列向量互相独立的实矩阵的摩尔－彭若斯广义逆：

$$
A^{+} = (A^{T}A)^{-1}A^{T}
$$

因此：

$$
\theta = X^{+}\vec{y}

 > 假如$$X^{T}X$$不可逆（列向量线性相关），一般有两种可能：
 > - 使用了两种近似的特征（比如房间大小和房间长度、宽度）
 > - 使用了过多特征。此时应减少特征数量，或采用一种后面会介绍的技术“正规化”。

### 比较梯度下降法和正规方程法

梯度下降法|正规方程法
----------|---------
需要选择学习速率|不需要选择学习速率
需要进行特征归一化|无需特征归一化
需要多次迭代|不需要迭代
$$ O(kn^2) $$|$$ O(n^3) $$，需要计算$$X^{T}X$$的逆
n很大时仍可行|n很大时运算速度很慢

## 多项式回归

之前我们都是解决线性回归问题，其实我们的回归函数未必一定是线性的。我们可以有二次回归函数、三次回归函数或者平方根回归函数等，使其变成不同的曲线的回归。

比如，原先我们有一个线性回归函数：

$$
h_\theta(x) = \theta_0 + \theta_1 x_1
$$

如果拟合情况不够好，我们完全可以将其改为三次函数：

$$
h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2 + \theta_3 x_1^3
$$

其实，这相当于我们新建了两个特征$$x_2$$和$$x_3$$：

$$
x_2 = x_1^2 and x_3 = x_1^3
$$

同样的我们可以尝试平方根回归函数：

$$
h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 \sqrt{x_1}
$$

相比二次函数，平方根函数的优势在于其单调递增的性质。  
因此对于数据范围不固定的数据，二次函数拟合后很有可能无法用于拟合新数据。

多项式回归参数仍然可以使用线性回归中的方法去求解，但一定要注意多项式回归的特征归一化：

如果$$x_1$$的数据范围是1 ~ 1000，那么$$x_1^2$$就有1 ~ 1000000的数据范围。
