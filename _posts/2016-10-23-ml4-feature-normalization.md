---
layout: post
title:  "机器学习（四）特征归一化"
image: ''
date:   2016-10-23 13:10:00
tags:
- computer science
- machine learning
- gradient descent
- normalization
description: '机器学习Coursera学习笔记：第四部分 特征归一化'
categories:
- Machine Learning
twitter_text: '机器学习Coursera学习笔记：第四部分 特征归一化'
---

## 简述

前面谈到了梯度下降算法，下降速度则与整个数据跨度有关。因此当存在多个特征时，如果特征数据范围不一致，可能会导致梯度下降的路径摇摆不定，效率低下。

因此我们需要进行特征归一化，简而言之，就是让他们基本在同一个数据范围内。

这包括两个方法：

 - 特征缩放
 - 平均归一化
 
## 特征缩放

让所有的特征的数据范围保持一致。只需某一特征的所有值除以其数据范围（最大值减去最小值）即可。

$$
x_i = \frac{x_i}{max\{x_i\} - min\{x_i\}}
$$

## 平均归一化

让所有特征数据以0为中心。只需某一特征的所有值减去其平均值即可。

$$
x_i = x_i - avg(x_i);
$$

## 总结

一般在梯度下降算法应用前，会同时使用以上两种方法预处理特征数据。即：

$$
x_i = \frac{x_i - \mu_i}{max\{x_i\} - min\{x_i\}}
$$

### 提示

到这里梯度下降算法就基本结束了，另外有几点提示：

 - 画出梯度下降过程代价函数值的变化图像，以便调试出合适的学习速率
 - 当迭代一次代价函数值减小不超过$$\epsilon$$时，就认为达到最小值点（$$\epsilon$$设为很小的值，比如$$10^{-3}$$）
